Meeting: May 15, 2021





1. Contribution to ongoing challenge - to be involved in similiar thinking: 

   1. Issue on scoring_tools related to imports: How to ensure a coherent scoring: e.g. from sklearn...import linear regression : Count 1 but if someone imports sklearn ...import linear regression - count of 2
   2. Measures of exploration/novelty/diversity: 
      1. Herfindhal index (Research Policy, Boudreau)
      2. Novelty measure (Nickerson) = landscape, entropy metrics

2. Reading of papers: 

   1. Reading of Boudreau
   2. Use the reading matrix as guideline: taking some notes

3. Research plan on the old data: 

   1. Going back to what we have

      1. 2017 to 2018

         1. What were the treatments?
         2. What needs to be cleaned for repos! 
         3. Data should be not all on GitHub: Were are the data: some on box? some on github? 

      2. General research question at time was: what is the effect of different types of transparency on performance (based experts)What is affect of transparency on reuse? 

         1. What are the data/what are measures?
            1. Survey
            2. Click data
            3. Code code data
         2. Measures
            1. Expert judges: 
               1. Technological quality
               2. Visusalization novelty 
         3. Simiarity: 
            1. Code Clone
            2. Visual similarity
            3. Function similarity (based libraries - functions)

         Â 